#  Hierarchical Decompositional Reinforcement Learning (HiDe)
This repository contains the code to implement the *Learning Functionally Decomposed Hierarchies for Continuous Navigation Tasks (HiDe)* algorithm. 
The paper is available at [OpenReview website](https://openreview.net/forum?id=r1nSxrKPH))

We are happy to answer any questions. Feel free to file an issue.

## Quick summary 
Quick summary and videos can be found on the project's website: https://sites.google.com/view/hide-rl

## Requirments
* Python 3.7
* PyTorch - unfortunately the night-build version is necessary due to a [bug](https://github.com/pytorch/pytorch/issues/24403). 
* NumPy
* MuJoCo - to obtain a license please follow: https://www.roboti.us/license.html
* MuJoCo
* scitkit-video - for rendering videos
* cv2 - for rendering videos with sigma overlay

## Citing
As of now, the paper is under review for ICLR20. Please use the BibTex generated by [OpenReview website](https://openreview.net/forum?id=r1nSxrKPH). 

## Reproduce experiments 
Each experiment is defined by a variant file. The variant names follow this pattern: `{AgentName}{MazeName}`. In order to change a maze, just replace the maze name.
To visualize live, add `--show` flag. To render a video, add `--save_video` to the command line.

### Show pretrained models
This repository contains pretrained models for all our experiments.

#### Experiment 1 - Simple Maze:
HiDe:

```
# Forward
python initialize_HAC.py --test --variant AntHiroMaze --exp_name HiDe_Simple_Ant --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --vpn --featurize_image --vpn_double_conv --Q_penalize --vpn_masking --no_attention --covariance --vpn_dqn --torch

# Backward
python initialize_HAC.py --test --variant AntHiroMazeBackwards --exp_name HiDe_Simple_Ant --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --vpn --featurize_image --vpn_double_conv --Q_penalize --vpn_masking --no_attention --covariance --vpn_dqn --torch

# Flipped
python initialize_HAC.py --test --variant AntHiroMazeFlipped --exp_name HiDe_Simple_Ant --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --vpn --featurize_image --vpn_double_conv --Q_penalize --vpn_masking --no_attention --covariance --vpn_dqn --torch

```

RelHAC

```
# Forward
python initialize_HAC.py --test --variant AntHiroMaze --exp_name RelHAC_Simple_Ant --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --torch --Q_penalize --torch

# Backward
python initialize_HAC.py --test --variant AntHiroMazeBackwards --exp_name RelHAC_Simple_Ant --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --torch --Q_penalize --torch

# Forward
python initialize_HAC.py --test --variant AntHiroMazeFlipped --exp_name RelHAC_Simple_Ant --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --torch --Q_penalize --torch
```

#### Experiment 2 - Complex Maze:
HiDe:

```
# Ant Forward
python initialize_HAC.py --test --variant AntWall --exp_name HiDe_Complex_Ant --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --vpn --featurize_image --vpn_double_conv --Q_penalize --vpn_masking --no_attention --covariance --vpn_dqn --torch

# Ant Backward
python initialize_HAC.py --test --variant AntWallBackwards --exp_name HiDe_Complex_Ant --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --vpn --featurize_image --vpn_double_conv --Q_penalize --vpn_masking --no_attention --covariance --vpn_dqn --torch

# Ant Flipped
python initialize_HAC.py --test --variant AntWallFlipped --exp_name HiDe_Complex_Ant --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --vpn --featurize_image --vpn_double_conv --Q_penalize --vpn_masking --no_attention --covariance --vpn_dqn --torch

```

```
# Ball Forward
python initialize_HAC.py --test --variant DMPointWall --exp_name HiDe_Complex_Ball --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --vpn --featurize_image --vpn_double_conv --Q_penalize --vpn_masking --no_attention --covariance --vpn_dqn --torch

# Ball Backward
python initialize_HAC.py --test --variant DMPointWallBackwards --exp_name HiDe_Complex_Ball --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --vpn --featurize_image --vpn_double_conv --Q_penalize --vpn_masking --no_attention --covariance --vpn_dqn --torch

# Ball Flipped
python initialize_HAC.py --test --variant DMPointWallFlipped --exp_name HiDe_Complex_Ball --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --vpn --featurize_image --vpn_double_conv --Q_penalize --vpn_masking --no_attention --covariance --vpn_dqn --torch

```

#### Experiment 3 - Transfer of agents for a complex environment:

```
# Forward B->A
python initialize_HAC.py --test --variant AntWallFlipped --exp_name HiDe_Complex_Ball2Ant --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --vpn --featurize_image --vpn_double_conv --Q_penalize --vpn_masking --no_attention --covariance --vpn_dqn --torch

# Backward B->A
python initialize_HAC.py --test --variant AntWallBackwards --exp_name HiDe_Complex_Ball2Ant --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --vpn --featurize_image --vpn_double_conv --Q_penalize --vpn_masking --no_attention --covariance --vpn_dqn --torch

# Flipped B->A
python initialize_HAC.py --test --variant AntWallFlipped --exp_name HiDe_Complex_Ball2Ant --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --vpn --featurize_image --vpn_double_conv --Q_penalize --vpn_masking --no_attention --covariance --vpn_dqn --torch

# Forward A->B
python initialize_HAC.py --test --variant DMPointWall --exp_name HiDe_Complex_Ant2Ball --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --vpn --featurize_image --vpn_double_conv --Q_penalize --vpn_masking --no_attention --covariance --vpn_dqn --torch

# Backward A->B
python initialize_HAC.py --test --variant DMPointWallBackwards --exp_name HiDe_Complex_Ant2Ball --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --vpn --featurize_image --vpn_double_conv --Q_penalize --vpn_masking --no_attention --covariance --vpn_dqn --torch

# Flipped A->B
python initialize_HAC.py --test --variant DMPointWallFlipped --exp_name HiDe_Complex_Ant2Ball --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --vpn --featurize_image --vpn_double_conv --Q_penalize --vpn_masking --no_attention --covariance --vpn_dqn --torch
```

### Retrain a model
The commands are the same as above, just replace `--test` with `--retrain`. Changing the experiment name is recommended as the command would overwrite the pretrained experiments.

#### Experiment 1 - Simple Maze:
HiDe:

```
python initialize_HAC.py --retrain --variant AntHiroMaze --exp_name HiDe_Simple_Ant_new --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --vpn --featurize_image --vpn_double_conv --Q_penalize --vpn_masking --no_attention --covariance --vpn_dqn --torch
```

RelHAC

```
python initialize_HAC.py --retrain --variant AntHiroMaze --exp_name RelHAC_Simple_Ant_new --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --torch --Q_penalize --torch
```

#### Experiment 2 - Complex Maze:
HiDe:

```
# Ant
python initialize_HAC.py --retrain --variant AntWall --exp_name HiDe_complex_Ant_new --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --vpn --featurize_image --vpn_double_conv --Q_penalize --vpn_masking --no_attention --covariance --vpn_dqn --high_penalty --torch

# Ball
python initialize_HAC.py --retrain --variant DMPointWall --exp_name HiDe_complex_Ant_new --num_Qs 1 --exp_num 1 --relative_subgoals --mask_global_info --vpn --featurize_image --vpn_double_conv --Q_penalize --vpn_masking --no_attention --covariance --vpn_dqn --high_penalty --torch
```

We found out that we get better generalization results when trained with `--high_penalty` flag.

#### Experiment 3 - Transfer of agents:
We provide the pretrained models both for Ant and Ball. Both were trained using RelHAC without the planning layer. 
Ant model has been taken from experiment 1, where as Point model was trained on Empty arena. You can find it in models/ directory together with scripts that transfer the weights.


## Credits
We would like to thank:
* [HAC](https://github.com/andrew-j-levy/Hierarchical-Actor-Critc-HAC-) Our codebase is based on theirs.
* [HIRO](https://github.com/tensorflow/models/tree/master/research/efficient-hrl) Our environments are based on their code.
* [DeepMind Control Suite](https://github.com/deepmind/dm_control/tree/master/dm_control/suite) Our Ball agent is inspired by their PointMass one.
